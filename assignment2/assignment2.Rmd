---
title: "Assignment2"
author: "Group 26: Aoming Sun,Siran Shen,Tong Pei"
date: "4 March 2025"
output: pdf_document
fontsize: 11pt
highlight: tango
---

## Exercise 1
```{r, fig.height=2, fig.width=4}
titanic = read.table("titanic.txt", header=TRUE)
titanic


```

 
 

## Exercise 2

**a)**
```{r, fig.height=2, fig.width=4}
coups = read.table("coups.txt", header=TRUE)
coups$pollib = factor(coups$pollib)
coupsglm = glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn + size + numelec + numregim, family = poisson, data = coups)
summary(coupsglm)
```


**b)**
To apply the step down strategy, we first remove variable numelec, which has the highest p-value.
```{r, fig.height=2, fig.width=4}
summary(glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn + size + numregim, family = poisson, data = coups))
```

We then remove variable numregim, which has the second highest p-value.
```{r, fig.height=2, fig.width=4}
summary(glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn + size, family = poisson, data = coups))
```

Next, we remove variable size, which has the third highest p-value.
```{r, fig.height=2, fig.width=4}
summary(glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn, family = poisson, data = coups))
```
We remove variable popn, which has the fourth highest p-value.
```{r, fig.height=2, fig.width=4}
summary(glm(miltcoup ~ oligarchy + pollib + parties + pctvote, family = poisson, data = coups))
```
We finally remove variable pctvote, which has the fifth highest p-value.
```{r, fig.height=2, fig.width=4}
summary(glm(miltcoup ~ oligarchy + pollib + parties, family = poisson, data = coups))
```

**c)** 
```{r, fig.height=2, fig.width=4}
coups$pollib = factor(coups$pollib)
coupsglm2 = glm(miltcoup ~ pollib + oligarchy + parties, family = poisson, data = coups)
summary(coupsglm2)

mean_oligarchy <- mean(coups$oligarchy)
mean_parties <- mean(coups$parties)

data_level1 = data.frame(pollib = "0", oligarchy = mean_oligarchy, parties = mean_parties)
data_level2 = data.frame(pollib = "1", oligarchy = mean_oligarchy, parties = mean_parties)
data_level3 = data.frame(pollib = "2", oligarchy = mean_oligarchy, parties = mean_parties)

predict(coupsglm2, data_level1, interval = "prediction")
predict(coupsglm2, data_level2, interval = "prediction")
predict(coupsglm2, data_level3, interval = "prediction")
```

## Exercise 3
a)
```{r, fig.height=2, fig.width=4}
library(MASS)
library(ggplot2)

data(stormer)

#Scatterplot of Time vs Weight colored by Viscosity
ggplot(stormer, aes(x = Wt, y = Time, color = factor(Viscosity))) +
  geom_point() +
  labs(x = "Weight", y = "Time", title = "Scatterplot of Stormer Data") 

ggplot(stormer, aes(x = Viscosity, y = Time, color = factor(Wt))) +
  geom_point() +
  labs(x = "Viscosity", y = "Time", title = "Scatterplot of Stormer Data") 
```


```{r, fig.height=2, fig.width=4}
#Fit linear model for initial parameter estimates
stormer$wT <- stormer$Wt * stormer$Time
init_model <- lm(wT ~ Viscosity + Time, data = stormer)

theta1_init <- coef(init_model)[["Viscosity"]]
theta2_init <- coef(init_model)[["Time"]]

#Nonlinear regression
form = as.formula(Time ~ (theta1 * Viscosity) / (Wt - theta2))
nls_model <- nls(
  form,
  data = stormer,
  start = list(theta1 = theta1_init, theta2 = theta2_init)
)
nls_model

theta_hat <- coef(nls_model)
print(theta_hat)
summary(nls_model)
sigma2_hat <- summary(nls_model)$sigma^2
print(sigma2_hat)
```

```{r, fig.height=2, fig.width=4}
#Generate predicted values for visualization
pred_grid <- expand.grid(
  Wt = seq(min(stormer$Wt), max(stormer$Wt), length.out = 100),
  Viscosity = unique(stormer$Viscosity)
)

pred_grid$Predicted <- (theta_hat["theta1"] * pred_grid$Viscosity) / 
  (pred_grid$Wt - theta_hat["theta2"])


ggplot(stormer, aes(x = Wt, y = Time, color = factor(Viscosity))) +
  geom_point() +
  geom_line(data = pred_grid, 
    aes(y = Predicted, color = factor(Viscosity)), linewidth = 0.8) +
  labs(x = "Weight", y = "Time", title = "Scatterplot of Stormer Data") 
```


```{r, fig.height=2, fig.width=4}
#Generate predicted values for visualization
pred_grid <- expand.grid(
#   Wt = seq(min(stormer$Wt), max(stormer$Wt), length.out = 5),
  Wt = unique(stormer$Wt),
  Viscosity = seq(min(stormer$Viscosity), max(stormer$Viscosity), length.out = 300)
)

pred_grid$Predicted <- (theta_hat["theta1"] * pred_grid$Viscosity) / 
  (pred_grid$Wt - theta_hat["theta2"])


ggplot(stormer, aes(x = Viscosity, y = Time, color = factor(Wt))) +
  geom_point() +
  geom_line(data = pred_grid, 
    aes(y = Predicted, color = factor(Wt)), linewidth = 0.8) +
  labs(x = "Weight", y = "Time", title = "Scatterplot of Stormer Data") 
```

b)
```{r}
theta1 <- 25
summary_nls <- summary(nls_model)

print(summary_nls)
print(summary_nls$df)
cov.est = vcov(nls_model)

# Get theta1 estimate and standard error
theta1_est <- coef(nls_model)["theta1"]
theta1_se <- summary_nls$coefficients["theta1", "Std. Error"]

# Residual degrees of freedom (n - p)
df <- summary_nls$df[2]

# Compute 95% confidence interval for theta1
#???????????????????by CI???
lb = theta1_est - qt(0.975, df) * sqrt(cov.est[1,1])
ub = theta1_est + qt(0.975, df) * sqrt(cov.est[1,1])
print(lb)
print(ub)
```

c)
 ```{r}
#comput by hand????????
confint(nls_model, level = 0.92, method = "asymptotic")
```
 
d)
```{r}
# Extract parameter estimates and variance-covariance matrix
theta_hat <- coef(nls_model)
vcov_theta <- vcov(nls_model)

# Define grid of viscosity values (v)
v_grid <- seq(10, 300, length.out = 100)
w_fixed <- 50  # Fixed weight

# Compute predictions, gradients, and standard errors
predictions <- data.frame(
  v = v_grid,
  T_hat = (theta_hat["theta1"] * v_grid) / (w_fixed - theta_hat["theta2"])
)

# Compute gradients (partial derivatives)
grad_theta1 <- v_grid / (w_fixed - theta_hat["theta2"])
grad_theta2 <- (theta_hat["theta1"] * v_grid) / (w_fixed - theta_hat["theta2"])^2
grad_matrix <- cbind(grad_theta1, grad_theta2)

# Calculate variance and standard error for each prediction
var_T <- diag(grad_matrix %*% vcov_theta %*% t(grad_matrix))
se_T <- sqrt(var_T)

# Compute 94% confidence intervals (asymptotic z-interval)
predictions$lower <- predictions$T_hat - qt(0.97, df) * se_T
predictions$upper <- predictions$T_hat + qt(0.97, df) * se_T

# Plot
ggplot(predictions, aes(x = v, y = T_hat)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, fill = "blue") +
  labs(
    title = "Expected Time (T) vs Viscosity (v) at Weight = 50",
    subtitle = "94% Confidence Intervals Using Asymptotic Normality",
    x = "Viscosity (v)", y = "Expected Time (T)"
  )
```

```{r}
## Validity of model assumptions
# residuals against the fitted values
plot(fitted(nls_model), resid(nls_model));
# qq-plot to check normality although normality is not required for nonlin models
qqnorm(resid(nls_model)); qqline(resid(nls_model),col="red") 
hist(resid(nls_model))  
```


e)
```{r}
form2 = as.formula(Time ~ (25 * Viscosity) / (Wt - theta2))
nls_model2 <- nls(
  form2,
  data = stormer,
  ###???redo theta2_init?
  start = list(theta2 = theta2_init)
)
nls_model2
anova(nls_model2, nls_model) # conclusion: reduced model is not adequate
```

```{r}

```
 
 
 
 
 
 
 
 
 
 