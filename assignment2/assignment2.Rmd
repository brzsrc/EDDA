---
title: "Assignment2"
author: "Group 26: Aoming Sun,Siran Shen,Tong Pei"
date: "4 March 2025"
output: pdf_document
fontsize: 11pt
highlight: tango
---

## Exercise 1
```{r, fig.height=2, fig.width=4}
library(ggplot2)

#a)

titanic = read.table("titanic.txt", header=TRUE)
titanic

# Load data and remove missing age entries 
titanic <- read.delim("titanic.txt")
titanic <- titanic[!is.na(titanic$Age), ]  # Drop rows with missing age

titanic$PClass <- factor(titanic$PClass)
titanic$Sex <- factor(titanic$Sex)

ggplot(titanic, aes(x = PClass, fill = factor(Survived))) + 
  geom_bar(position = "fill") + 
  facet_wrap(~ Sex) + 
  labs(title = "Survival Rate by Class & Gender", 
       x = "Passenger Class", 
       fill = "Survived (0=No, 1=Yes)")

# 2. Age distribution (simple boxplot)
ggplot(titanic, aes(x = factor(Survived), y = Age)) + 
  geom_boxplot() + 
  labs(x = "Survived (0=No, 1=Yes)", 
       title = "Age Distribution by Survival")


# Fit logistic regression (no interaction)
model <- glm(Survived ~ PClass + Age + Sex, 
             data = titanic, 
             family = binomial)

# Essential outputs
cat(" Model Summary ")
summary(model)  # Show coefficients and p-values

cat(" Odds Ratios ")
odds_ratio <- exp(coef(model))  
print(odds_ratio)
```
Given the OR as above, we can conclude that comparing to 1st class, the probability of survival of the same person in 2nd class will be 27% and 8% in third class. Comparing being a female, a man can only survive 7%. And every year older of a passenger will decrease around 4% survival probability.

```{r, fig.height=2, fig.width=4}
#b)
# main model
model_main <- glm(Survived ~ PClass + Age + Sex, 
                 data = titanic, family = binomial)

# model with age and class
model_age_class <- glm(Survived ~ PClass * Age + Sex, 
                      data = titanic, family = binomial)

# model with age and sexuality
model_age_sex <- glm(Survived ~ PClass + Age * Sex, 
                    data = titanic, family = binomial)

# LRT
cat("---- Testing PClass*Age Interaction ----\n")
anova(model_main, model_age_class, test = "LRT")  # p>0.05则不显著

cat("\n---- Testing Age*Sex Interaction ----\n")
anova(model_main, model_age_sex, test = "LRT")    # p<0.05则显著


```
Here we get the output of the LRT. we can see the p value of LRT in Age * Sex is much smaller than 0.05 while Class * Age is a bit more than 0.05. Which means the Age * Sex model brought more significant improvements. And we are choosing this model (model_age_sex) because it gives lower Residual Deviance.

```{r, fig.height=2, fig.width=4}
# let Age * Sex be the new model
final_model <- model_age_sex


# prediction of 55

# generate new data
new_data <- expand.grid(
  PClass = factor(c("1st", "2nd", "3rd"), levels = c("1st", "2nd", "3rd")),
  Sex = factor(c("male", "female"), levels = c("male", "female")),
  Age = 55  # set all ages 55
)

# predict
new_data$Survival_Prob <- predict(final_model, newdata = new_data, 
                                 type = "response")

print(new_data)
```
In the table above we can see the same trend, woman are more likely to survive, higher class are more likely to servive.


```{r, fig.height=2, fig.width=4}
```
#c) When we use the model_age_sex as the prediction model we can measure the quality of the prediction by AUC of ROC.

ROC is a curve where x axis is False Positive Rate and Y is True Positive Rate. The Area under curve of ROC is between 0 to 1. the bigger value of AUC meaning the model performs better. 0.5 is the standard line of random guessing.

In practice we can break the dataset into train and test set where train take about 80%. Once the model is trained we can use the model to predict the test set and apply ROC curve to evaluate the quality of the trained model. 

Sometimes random selection of train or test set may lead to imbalance, thus we can do n-fold. break the whole data set into 10 random equal size small sets. Now we take random 2 set as test set the rest as the training set. We repeat this step until all the small set have been selected as test set. Then we have a list of trained model and we can find the best one. It's less likely get affected by data imbalance.

```{r, fig.height=2, fig.width=4}


#d)

# create contingency table of class 
table_pclass <- table(titanic_clean$PClass, titanic_clean$Survived)
print("Contingency Table (PClass vs Survived):")
print(table_pclass)

# Do the test
chi_pclass <- chisq.test(table_pclass)

# check the expected frequency
cat("\nExpected Counts (PClass):\n")
print(chi_pclass$expected)
# check the observed frequency
cat("\nChi-squared Test for PClass:\n")
print(chi_pclass)


# create contingency table of sex
table_sex <- table(titanic_clean$Sex, titanic_clean$Survived)
print("\nContingency Table (Sex vs Survived):")
print(table_sex)

# Do the test
chi_sex <- chisq.test(table_sex)

# check the expected frequency
cat("\nExpected Counts (Sex):\n")
print(chi_sex$expected)
# check the observed frequency
cat("\nChi-squared Test for Sex:\n")
print(chi_sex)


```
Given the tests above we can say that both the class and sex are significantly related to survival rates. 



```{r, fig.height=2, fig.width=4}
```
#e) The method in d is not wrong but only suitable for checking the dependence of 2 factors. While the LRT can also compare the interaction between 2 factors. 

The advantage of contingency table test is simple can be manually calculated by counting the frequency. however the disadvantage is that it can only focus on 2 factors each test not more and not the interaction of factors. 

The advantage of LRT can test more complicate dependence. However, it surely takes more steps and computing power. 

If we have multiple factor pairs need to compare contingency table test is more efficient.

If we need to test the complex dependence between 2 factors, LRT is better.
 
```{r, fig.height=2, fig.width=4}



## Exercise 2
```
**a)**
```{r, fig.height=2, fig.width=4}
coups = read.table("coups.txt", header=TRUE)
coups$pollib = factor(coups$pollib)
coupsglm = glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn + size + numelec + numregim, family = poisson, data = coups)
summary(coupsglm)
```


**b)**
To apply the step down strategy, we first remove variable numelec, which has the highest p-value.
```{r, fig.height=2, fig.width=4}
summary(glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn + size + numregim, family = poisson, data = coups))
```

We then remove variable numregim, which has the second highest p-value.
```{r, fig.height=2, fig.width=4}
summary(glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn + size, family = poisson, data = coups))
```

Next, we remove variable size, which has the third highest p-value.
```{r, fig.height=2, fig.width=4}
summary(glm(miltcoup ~ oligarchy + pollib + parties + pctvote + popn, family = poisson, data = coups))
```
We remove variable popn, which has the fourth highest p-value.
```{r, fig.height=2, fig.width=4}
summary(glm(miltcoup ~ oligarchy + pollib + parties + pctvote, family = poisson, data = coups))
```
We finally remove variable pctvote, which has the fifth highest p-value.
```{r, fig.height=2, fig.width=4}
summary(glm(miltcoup ~ oligarchy + pollib + parties, family = poisson, data = coups))
```

**c)** 
```{r, fig.height=2, fig.width=4}
coups$pollib = factor(coups$pollib)
coupsglm2 = glm(miltcoup ~ pollib + oligarchy + parties, family = poisson, data = coups)
summary(coupsglm2)

mean_oligarchy <- mean(coups$oligarchy)
mean_parties <- mean(coups$parties)

data_level1 = data.frame(pollib = "0", oligarchy = mean_oligarchy, parties = mean_parties)
data_level2 = data.frame(pollib = "1", oligarchy = mean_oligarchy, parties = mean_parties)
data_level3 = data.frame(pollib = "2", oligarchy = mean_oligarchy, parties = mean_parties)

predict(coupsglm2, data_level1, interval = "prediction")
predict(coupsglm2, data_level2, interval = "prediction")
predict(coupsglm2, data_level3, interval = "prediction")
```

## Exercise 3
a)
```{r, fig.height=6, fig.width=6}
library(MASS)
library(ggplot2)
library("scatterplot3d")

data(stormer)

#Scatterplot of Time vs Weight colored by Viscosity
ggplot(stormer, aes(x = Wt, y = Time, color = factor(Viscosity))) +
  geom_point() +
  labs(x = "Weight", y = "Time", title = "Scatterplot of Stormer Data") 

ggplot(stormer, aes(x = Viscosity, y = Time, color = factor(Wt))) +
  geom_point() +
  labs(x = "Viscosity", y = "Time", title = "Scatterplot of Stormer Data") 


# scatterplot3d(stormer[,1:3], angle = 55,
#               grid=TRUE, box=FALSE)

```


```{r, fig.height=2, fig.width=4}
#Fit linear model for initial parameter estimates
stormer$wT <- stormer$Wt * stormer$Time
init_model <- lm(wT ~ Viscosity + Time, data = stormer)

theta1_init <- coef(init_model)[["Viscosity"]]
theta2_init <- coef(init_model)[["Time"]]

#Nonlinear regression
form = as.formula(Time ~ (theta1 * Viscosity) / (Wt - theta2))
nls_model <- nls(
  form,
  data = stormer,
  start = list(theta1 = theta1_init, theta2 = theta2_init)
)
nls_model

theta_hat <- coef(nls_model)
print(theta_hat)
summary(nls_model)
sigma2_hat <- summary(nls_model)$sigma^2
print(sigma2_hat)
```

```{r, fig.height=2, fig.width=4}
#Generate predicted values for visualization
pred_grid <- expand.grid(
  Wt = seq(min(stormer$Wt), max(stormer$Wt), length.out = 100),
  Viscosity = unique(stormer$Viscosity)
)

pred_grid$Predicted <- (theta_hat["theta1"] * pred_grid$Viscosity) / 
  (pred_grid$Wt - theta_hat["theta2"])


ggplot(stormer, aes(x = Wt, y = Time, color = factor(Viscosity))) +
  geom_point() +
  geom_line(data = pred_grid, 
    aes(y = Predicted, color = factor(Viscosity)), linewidth = 0.8) +
  labs(x = "Weight", y = "Time", title = "Scatterplot of Stormer Data") 
```


```{r, fig.height=2, fig.width=4}
#Generate predicted values for visualization
pred_grid <- expand.grid(
#   Wt = seq(min(stormer$Wt), max(stormer$Wt), length.out = 5),
  Wt = unique(stormer$Wt),
  Viscosity = seq(min(stormer$Viscosity), max(stormer$Viscosity), length.out = 300)
)

pred_grid$Predicted <- (theta_hat["theta1"] * pred_grid$Viscosity) / 
  (pred_grid$Wt - theta_hat["theta2"])


ggplot(stormer, aes(x = Viscosity, y = Time, color = factor(Wt))) +
  geom_point() +
  geom_line(data = pred_grid, 
    aes(y = Predicted, color = factor(Wt)), linewidth = 0.8) +
  labs(x = "Weight", y = "Time", title = "Scatterplot of Stormer Data") 
```

b)
```{r}
theta1 <- 25
summary_nls <- summary(nls_model)

print(summary_nls)
print(summary_nls$df)
cov.est = vcov(nls_model)

# Get theta1 estimate and standard error
theta1_est <- coef(nls_model)["theta1"]
theta1_se <- summary_nls$coefficients["theta1", "Std. Error"]

# Residual degrees of freedom (n - p)
df <- summary_nls$df[2]

# Compute 95% confidence interval for theta1
#???????????????????by CI???
lb = theta1_est - qt(0.975, df) * sqrt(cov.est[1,1])
ub = theta1_est + qt(0.975, df) * sqrt(cov.est[1,1])
print(lb)
print(ub)
```

c)
 ```{r}
#comput by hand????????
confint(nls_model, level = 0.92, method = "asymptotic")
```
 
d)
```{r}
# Extract parameter estimates and variance-covariance matrix
theta_hat <- coef(nls_model)
vcov_theta <- vcov(nls_model)

# Define grid of viscosity values (v)
v_grid <- seq(10, 300, length.out = 100)
w_fixed <- 50  # Fixed weight

# Compute predictions, gradients, and standard errors
predictions <- data.frame(
  v = v_grid,
  T_hat = (theta_hat["theta1"] * v_grid) / (w_fixed - theta_hat["theta2"])
)

# Compute gradients (partial derivatives)
grad_theta1 <- v_grid / (w_fixed - theta_hat["theta2"])
grad_theta2 <- (theta_hat["theta1"] * v_grid) / (w_fixed - theta_hat["theta2"])^2
grad_matrix <- cbind(grad_theta1, grad_theta2)

# Calculate variance and standard error for each prediction
var_T <- diag(grad_matrix %*% vcov_theta %*% t(grad_matrix))
se_T <- sqrt(var_T)

# Compute 94% confidence intervals (asymptotic z-interval)
predictions$lower <- predictions$T_hat - qt(0.97, df) * se_T
predictions$upper <- predictions$T_hat + qt(0.97, df) * se_T

# Plot
ggplot(predictions, aes(x = v, y = T_hat)) +
  geom_line(color = "blue", linewidth = 1) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, fill = "blue") +
  labs(
    title = "Expected Time (T) vs Viscosity (v) at Weight = 50",
    subtitle = "94% Confidence Intervals Using Asymptotic Normality",
    x = "Viscosity (v)", y = "Expected Time (T)"
  )
```

```{r}
## Validity of model assumptions
# residuals against the fitted values
plot(fitted(nls_model), resid(nls_model));
# qq-plot to check normality although normality is not required for nonlin models
qqnorm(resid(nls_model)); qqline(resid(nls_model),col="red") 
hist(resid(nls_model))  
```


e)
```{r}
form2 = as.formula(Time ~ (25 * Viscosity) / (Wt - theta2))
nls_model2 <- nls(
  form2,
  data = stormer,
  ###???redo theta2_init?
  start = list(theta2 = theta2_init)
)
nls_model2
anova(nls_model2, nls_model) # conclusion: reduced model is not adequate
```

```{r}

```
 
 
 
 
 
 
 
 
 
 